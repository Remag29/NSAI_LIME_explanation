{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet 2 : Diabetes Prediction explained with SHAP\n",
    "\n",
    "## Data explanation\n",
    "\n",
    "In AI, it is important to understand the model's predictions.\n",
    "In majority of cases, the model is a black box and it is difficult to understand how the model makes its predictions.\n",
    "In this project, we will use SHAP and LIME to explain the predictions of a diabetes prediction model.\n",
    "\n",
    "LIME focuses on locally perturbing data to understand how a model makes decisions. It generates perturbed examples around a specific instance and observes how the model reacts to these changes. By fitting a simple model to this perturbed data, LIME creates a local explanation for the model's decision.\n",
    "\n",
    "On the other hand, SHAP relies on game theory to assign a contribution of each feature to the model prediction. It calculates Shapley values, which represent the marginal contribution of each feature in the prediction. SHAP offers a global explanation by taking into account all the characteristics.\n",
    "\n",
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this project is the [diabetes2](https://www.kaggle.com/datasets/cjboat/diabetes2) dataset from Kaggle.\n",
    "\n",
    "The dataset contains 9 columns and 768 rows. The columns are:\n",
    "- Pregnancies\n",
    "- Glucose\n",
    "- BloodPressure\n",
    "- SkinThickness\n",
    "- Insulin\n",
    "- BMI\n",
    "- DiabetesPedigreeFunction\n",
    "- Age\n",
    "- Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "diabetes_data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "diabetes_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "A Random Forest classifier is built to predict diabetes outcomes using the diabetes dataset.\n",
    "\n",
    "For the parameters of the Random Forest classifier, we limit the maximum features to 2. We have set the number of trees to 100. More the number of trees, the better the model will be. However, it will take more time to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load useful libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Separate Features and Target Variables\n",
    "X = diabetes_data.drop(columns='Outcome')\n",
    "y = diabetes_data['Outcome']\n",
    "\n",
    "# Create Train & Test Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,\n",
    "                                                \tstratify =y,\n",
    "                                                \trandom_state = 13)\n",
    "\n",
    "# Build the model\n",
    "rf_clf = RandomForestClassifier(max_features=2, n_estimators =100 ,bootstrap = True)\n",
    "\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction on the testing data\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is not very accurate. The f1-score is `0.78`, which is correct but not excellent.\n",
    "But it is enough to explain the model with SHAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a SHAP explainer to explain the Random Forest classifier.\n",
    "We apply the SHAP explainer on the `X_test` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load JS visualization code to notebook\n",
    "shap.initjs()\n",
    "\n",
    "# Create the explainer\n",
    "explainer = shap.TreeExplainer(rf_clf)\n",
    "\n",
    "shap_values = explainer.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most important features\n",
    "\n",
    "In this plot, we are ranked the features by their importance. The most important feature is `Glucose`, followed by `BMI` and `Age`.\n",
    "\n",
    "That means that the `Glucose` level is the most important feature to predict diabetes outcomes.\n",
    "\n",
    "We can also see that the importance of the features does not change much depending on the class. The most important features are the same for both classes.\n",
    "\n",
    "This plot also shows that the `BloodPressure` and `SkinThickness` features are not important to predict diabetes outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Variable Importance Plot - Global Interpretation\")\n",
    "figure = plt.figure()\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print another plot to see the SHAP values for each feature of the class 1.\n",
    "To do this we use the `shap_values[1]` array, that contains the SHAP values for the class 1.\n",
    "\n",
    "As we can see, the plot represent all the features points and the positive or negative impact of each feature on the prediction.\n",
    "\n",
    "Points in red are the features that have a positive impact on the prediction, and points in blue are the features that have a negative impact on the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[1], X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lime Tabular Explainer\n",
    "\n",
    "It is also possible to use the Lime Tabular Explainer to explain the Random Forest classifier.\n",
    "This visualizes the most important features for the prediction of diabetes outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LimeTabularExplainer module\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# Get the class names\n",
    "class_names = ['Has diabetes', 'No diabetes']\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = list(X_train.columns)\n",
    "\n",
    "# Fit the Explainer on the training data set using the LimeTabularExplainer\n",
    "explainer = LimeTabularExplainer(X_train.values, feature_names =     \n",
    "                                 feature_names,\n",
    "                                 class_names = class_names, \n",
    "                                 mode = 'classification')\n",
    "\n",
    "# Get the explanation for a specific instance\n",
    "instance = X_test.values[86]\n",
    "\n",
    "print(instance)\n",
    "\n",
    "explaination = explainer.explain_instance(instance, rf_clf.predict_proba, top_labels = 1)\n",
    "\n",
    "# Plot the explaination\n",
    "explaination.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "dt_clf = DecisionTreeClassifier(max_depth = 3, min_samples_leaf = 2)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data and evaluate the model\n",
    "y_pred = dt_clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = plot_tree(dt_clf,\n",
    "               \tfeature_names = feature_names,\n",
    "               \tclass_names = class_names,\n",
    "               \tfilled=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
